{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Amazon Sentiment Analysis with word Polarity-Orignal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MukulKirtiVerma/Negtive-Positive-Word-List/blob/master/Amazon_Sentiment_Analysis_with_word_Polarity_Orignal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OZeBefSNJXRF"
      },
      "source": [
        "#!pip install scikit-plot\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from sklearn.model_selection import *\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from matplotlib import pyplot\n",
        "import gc\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve as learning_cv\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import validation_curve\n",
        "import scikitplot as skplt   \n",
        "from sklearn.metrics import classification_report   \n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import roc_curve \n",
        "from sklearn.metrics import auc\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras import models, layers, optimizers\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import bz2\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score,confusion_matrix\n",
        "import re\n",
        "%matplotlib inline\n",
        "import os\n",
        "# import call method from subprocess module \n",
        "from subprocess import call \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import bz2\n",
        "import re\n",
        "#from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "#import spacy\n",
        "from numba import jit, cuda \n",
        "def get_labels_and_texts(file):\n",
        "    labels = []\n",
        "    texts = []\n",
        "    for line in bz2.BZ2File(file):\n",
        "        x = line.decode(\"utf-8\")\n",
        "        labels.append(int(x[9]) - 1)\n",
        "        texts.append(x[10:].strip())\n",
        "    return np.array(labels), texts\n",
        "y_train, train_texts= get_labels_and_texts('/content/drive/My Drive/test.ft.txt.bz2')\n",
        "y_train, train_texts = y_train[:100000], train_texts[:100000]\n",
        "print(\"data readed\")\n",
        "gc.collect()\n",
        "xx=[]\n",
        "import re\n",
        "for i in train_texts:\n",
        "    i=i.lower()\n",
        "    i=i.replace('.',' ')\n",
        "    i=i.replace('`',' ')\n",
        "    i=i.replace('~',' ')\n",
        "    i=i.replace('@',' ')\n",
        "    i=i.replace('#',' ')\n",
        "    i=i.replace('$',' ')\n",
        "    i=i.replace('%',' ')\n",
        "    i=i.replace('^',' ')\n",
        "    i=i.replace('&',' ')\n",
        "    i=i.replace('*',' ')\n",
        "    i=i.replace('(',' ')\n",
        "    i=i.replace(')',' ')\n",
        "    i=i.replace('_',' ')\n",
        "    i=i.replace('-',' ')\n",
        "    i=i.replace('+',' ')\n",
        "    i=i.replace('=',' ')\n",
        "    i=i.replace('{',' ')\n",
        "    i=i.replace('}',' ')\n",
        "    i=i.replace('[',' ')\n",
        "    i=i.replace(']',' ')\n",
        "    i=i.replace('|',' ')\n",
        "    i=i.replace(\"\\\\\",' ')\n",
        "    i=i.replace(':',' ')\n",
        "    i=i.replace(';',' ')\n",
        "    i=i.replace('\\\"',' ')\n",
        "    i=i.replace('/',' ')\n",
        "\n",
        "    i=i.replace('?',' ')\n",
        "    i=i.replace('<',' ')\n",
        "    i=i.replace(',',' ')\n",
        "    i=re.sub(\"n't\",' not',i)\n",
        "    i=re.sub(\"'ll\",' will',i)\n",
        "    i=re.sub(\"'m\",' am',i)\n",
        "    i=re.sub(\"'s\",\" is\",i)\n",
        "    i=re.sub(\"'re\",\" are\",i)\n",
        "    i=re.sub(\"'ve\",\" have\",i)\n",
        "\n",
        "    i=re.sub(\"'d\",\" would\",i)\n",
        "    i=re.sub(\"'\",\" \",i)\n",
        "    i=i.replace(r\"\\n\",\" \")\n",
        "\n",
        "    i=i.replace('>',' ')\n",
        "    i=i.replace('!',' ')\n",
        "\n",
        "\n",
        "    i=i.replace('>',' ')\n",
        "    i=i.replace('_',' ')\n",
        "    i=i.replace(' br ',' ')\n",
        "    i=re.sub(' +',' ',i)\n",
        "    i=re.sub('\\b[a-z]+\\b',' ',i)\n",
        "    xx.append(i)\n",
        "x_traint=xx\n",
        "del xx\n",
        "gc.collect()\n",
        "print(\"data refine done\")\n",
        "\n",
        "# imported the requests library \n",
        "import requests \n",
        "data_url = \"https://github.com/MukulKirtiVerma/Negtive-Positive-Word-List/raw/master/Positive%20and%20Negative%20Word%20List.xlsx\"\n",
        "\n",
        "# URL of the image to be downloaded is defined as image_url \n",
        "r = requests.get(data_url) # create HTTP response object \n",
        "\n",
        "# send a HTTP request to the server and save \n",
        "# the HTTP response in a response object called r \n",
        "with open(\"data_file.xlsx\",'wb') as f: \n",
        "\n",
        "\t# Saving received content as a png file in \n",
        "\t# binary format \n",
        "\n",
        "\t# write the contents of the response (r.content) \n",
        "\t# to a new file in binary mode. \n",
        "\tf.write(r.content) \n",
        "df=pd.read_excel('/content/data_file.xlsx')\n",
        "df.head()\n",
        "pw=list(df['Positive Sense Word List'])\n",
        "nw=list(df['Negative Sense Word List'])\n",
        "\n",
        "print('pw nw read')\n",
        "\n",
        "def load_data(Train_df,y,idx,batch_size):\n",
        "    global fl\n",
        "    global model_w\n",
        "    global nw\n",
        "    global pw\n",
        "    global word_data\n",
        "    x_t=[]\n",
        "    for i in range(idx*batch_size,idx*batch_size+batch_size):\n",
        "        try:\n",
        "            #print(i,len(Train_df),len(y),idx,batch_size)\n",
        "            w=np.zeros((100,len(fl)))\n",
        "            train=Train_df[i]\n",
        "            train=train.split(' ')\n",
        "            for j in train:\n",
        "\n",
        "                if j in fl:\n",
        "                    if j in nw:\n",
        "                        w[:,fl.index(j)]=np.asarray(minmax_scale(model_w.wv.word_vec(j),feature_range=(-1,-0.9999)))    \n",
        "                    elif j in pw:\n",
        "                        w[:,fl.index(j)]=np.asarray(minmax_scale(model_w.wv.word_vec(j),feature_range=(0,1)))    \n",
        "                    else:\n",
        "                        w[:,fl.index(j)]=np.asarray(model_w.wv.word_vec(j))\n",
        "        except: \n",
        "            break\n",
        "        x_t.append(w)\n",
        "        \n",
        "    return (np.asarray(x_t), np.asarray(y[idx*batch_size:idx*batch_size+batch_size]))\n",
        "def batch_generator(Train_df,y,batch_size,steps):\n",
        "    idx=1\n",
        "    while True: \n",
        "        yield load_data(Train_df,y,idx-1,batch_size)## Yields data\n",
        "        print(idx,steps,end='')\n",
        "        if idx<steps:\n",
        "            idx+=1\n",
        "        else:\n",
        "            idx=1\n",
        "            break\n",
        "\n",
        "word_dict={}\n",
        "for i in x_traint:\n",
        "    for word in i.split(' '):\n",
        "        try:\n",
        "            word_dict[word]=word_dict[word]+1\n",
        "        except:\n",
        "            word_dict[word]=1\n",
        "sort_orders = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "s_d=dict(sort_orders)\n",
        "\n",
        "\n",
        "fl=[]\n",
        "for i in s_d:\n",
        "    if(s_d[i]<=8):\n",
        "        if(i in pw or i in nw):\n",
        "            fl.append(i)\n",
        "    else:\n",
        "        fl.append(i)\n",
        "print(len(fl))\n",
        "\n",
        "\n",
        "word_data=[]\n",
        "word_data=[i.split(' ') for i in x_traint]\n",
        "print(\"word 2 vec started\")\n",
        "\n",
        "import numpy as np\n",
        "# train model\n",
        "model_w = Word2Vec(word_data,size=100, min_count=1)\n",
        "#x_train=word_to_vec_with_polarity(x_train,model_w)\n",
        "gc.collect()\n",
        "print('word2vec done')\n",
        "\n",
        "def load_data_glove(Train_df,y,idx,batch_size):\n",
        "    global fl\n",
        "    global model_g\n",
        "    global nw\n",
        "    global pw\n",
        "    global word_data\n",
        "    x_t=[]\n",
        "    for i in range(idx*batch_size,idx*batch_size+batch_size):\n",
        "        try:\n",
        "            #print(i,len(Train_df),len(y),idx,batch_size)\n",
        "            w=np.zeros((100,len(fl)))\n",
        "            train=Train_df[i]\n",
        "            train=train.split(' ')\n",
        "            for j in train: \n",
        "                try:\n",
        "                    if j in fl:\n",
        "                        if j in nw:\n",
        "                            w[:,fl.index(j)]=np.asarray(minmax_scale(model_g[j],feature_range=(-1,-0.9999)))    \n",
        "                        elif j in pw:\n",
        "                            w[:,fl.index(j)]=np.asarray(minmax_scale(model_g[j],feature_range=(0,1)))    \n",
        "                        else:\n",
        "                            w[:,fl.index(j)]=np.asarray(model_g[j])\n",
        "                except:\n",
        "                    pass\n",
        "        except: \n",
        "            break\n",
        "        x_t.append(w)\n",
        "        \n",
        "    return (np.asarray(x_t), np.asarray(y[idx*batch_size:idx*batch_size+batch_size]))\n",
        "def batch_generator_glove(Train_df,y,batch_size,steps):\n",
        "    idx=1\n",
        "    while True: \n",
        "        yield load_data_glove(Train_df,y,idx-1,batch_size)## Yields data\n",
        "        print(idx,steps,end='')\n",
        "        if idx<steps:\n",
        "            idx+=1\n",
        "        else:\n",
        "            idx=1\n",
        "            break\n",
        "print('glove started')\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = '/content/drive/My Drive/imdb analysis/glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "from gensim.models import KeyedVectors\n",
        "# load the Stanford GloVe model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "model_g = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
        "print('glove model fit')\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "batch_size=70\n",
        "training_size=int((len(y_train)*70)/100)\n",
        "val_size=int((training_size*20)/100)\n",
        "test_size=int((len(y_train)*30)/100)\n",
        "steps_per_epoch=int(training_size/batch_size)\n",
        "val_steps_per_epoch=int(val_size/batch_size)\n",
        "test_steps_per_epoch=int(test_size/batch_size)\n",
        "x_step=val_steps_per_epoch\n",
        "\n",
        "def training_process(model):\n",
        "    global y_train\n",
        "    global x_traint\n",
        "    global x_step\n",
        "    batch_size=70\n",
        "    training_size=int((len(y_train)*70)/100)\n",
        "    val_size=int((training_size*20)/100)\n",
        "    test_size=int((len(y_train)*30)/100)\n",
        "    steps_per_epoch=int(training_size/batch_size)\n",
        "    val_steps_per_epoch=int(val_size/batch_size)\n",
        "    test_steps_per_epoch=int(test_size/batch_size)\n",
        "    \n",
        "    x_train=batch_generator(x_traint[:training_size],y_train[:training_size],batch_size,steps_per_epoch)\n",
        "    x_val=batch_generator(x_traint[training_size:training_size+val_size],y_train[training_size:training_size+val_size],batch_size,x_step)\n",
        "    history=model.fit(x_train,\n",
        "     epochs=1,steps_per_epoch=steps_per_epoch, \n",
        "     verbose=1, validation_data=x_val,\n",
        "     validation_steps=val_steps_per_epoch)\n",
        "    # Plot the training loss \n",
        "    #plt.plot(history.history['accuracy'])\n",
        "    # Plot the validation loss\n",
        "    #plt.plot(history.history['val_accuracy'])\n",
        "    # Show the figure\n",
        "    #plt.show()\n",
        "    # Plot the training loss \n",
        "    #plt.plot(history.history['loss'])\n",
        "    #plt.plot(history.history['val_loss'])\n",
        "    # Show the figure\n",
        "    #plt.show()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhELZVmI9yP_"
      },
      "source": [
        "def training_process_glove(model):\n",
        "    global y_train\n",
        "    global x_traint\n",
        "    global x_step\n",
        "\n",
        "    batch_size=70\n",
        "    training_size=int((len(y_train)*70)/100)\n",
        "    val_size=int((training_size*20)/100)\n",
        "    test_size=int((len(y_train)*30)/100)\n",
        "    steps_per_epoch=int(training_size/batch_size)\n",
        "    val_steps_per_epoch=int(val_size/batch_size)\n",
        "    test_steps_per_epoch=int(test_size/batch_size)\n",
        "\n",
        "    \"\"\"\n",
        "    training_size=28000\n",
        "    val_size=7000\n",
        "    test_size=15000\"\"\"\n",
        "    \n",
        "    \n",
        "    x_train=batch_generator_glove(x_traint[:training_size],y_train[:training_size],batch_size,steps_per_epoch)\n",
        "    x_val=batch_generator_glove(x_traint[training_size:training_size+val_size],y_train[training_size:training_size+val_size],batch_size,x_step)\n",
        "    history=model.fit_generator(x_train,\n",
        "     epochs=1,steps_per_epoch=steps_per_epoch,\n",
        "     verbose=1, validation_data=x_val,\n",
        "     validation_steps=val_steps_per_epoch)\n",
        "    # Plot the training loss \n",
        "    plt.plot(history.history['accuracy'])\n",
        "    # Plot the validation loss\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    # Show the figure\n",
        "    plt.show()\n",
        "    # Plot the training loss \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    # Show the figure\n",
        "    plt.show()\n",
        "    return model\n",
        "\n",
        "def ploting(y_pred,y_test,y_proba,title):\n",
        "    #confmatrix\n",
        "    #confmatrix\n",
        "    #ax = plt.axes()\n",
        "    if(type(y_pred)==type([1,2,3])):\n",
        "      y_pred=np.asarray(y_pred)\n",
        "    confusion_mat=confusion_matrix(y_test,y_pred)\n",
        "    print(\"TPR :\" ,confusion_mat[0][0]/(confusion_mat[0][0]+confusion_mat[0][1]))\n",
        "    print(\"FPR :\" ,confusion_mat[1][0]/(confusion_mat[1][0]+confusion_mat[1][1]))\n",
        "    m = tf.keras.metrics.Precision()\n",
        "    _ = m.update_state(y_test,y_pred.reshape(y_test.shape[0],-1))\n",
        "    precision=m.result().numpy()\n",
        "    m = tf.keras.metrics.Recall()\n",
        "    _ = m.update_state(y_test,y_pred.reshape(y_test.shape[0],-1))\n",
        "    recall=m.result().numpy() \n",
        "    print(\"precision :\", precision)\n",
        "    print(\"recall :\", recall)\n",
        "    print('Accuracy score:{}'.format(accuracy_score(y_test, y_pred)))\n",
        "    print('F1 score :{}'.format(f1_score(y_test, y_pred)))\n",
        "    print('ROC AUC score:{}'.format(roc_auc_score(y_test, y_pred)))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    ax = plt.axes()\n",
        "    confusion_mat=confusion_matrix(y_test,y_pred)\n",
        "    sns.heatmap(confusion_mat, annot=True, fmt=\"d\",ax = ax)\n",
        "    ax.set_title('Confusion matrix with '+title+'\\n')\n",
        "    plt.show() \n",
        "    skplt.metrics.plot_roc(y_test, y_proba,plot_micro=False) \n",
        "    plt.title(\"ROC Curve with \"+title+\"\\n\")\n",
        "    plt.figure(figsize=(30,30))\n",
        "    plt.show()\n",
        "    skplt.metrics.plot_precision_recall(y_test, y_proba)\n",
        "    plt.title(\"Precision Recall Curve with \" +title+\"\\n\")\n",
        "    plt.show()\n",
        "    \n",
        "test_len=0\n",
        "test_len2=0\n",
        "def test_result(model):\n",
        "    global y_train\n",
        "    global x_traint\n",
        "    batch_size=70\n",
        "    training_size=int((len(y_train)*70)/100)\n",
        "    val_size=int((training_size*20)/100)\n",
        "    test_size=int((len(y_train)*30)/100)\n",
        "    steps_per_epoch=int(training_size/batch_size)\n",
        "    val_steps_per_epoch=int(val_size/batch_size)\n",
        "    test_steps_per_epoch=int(test_size/batch_size)\n",
        "\n",
        "    x_train=batch_generator(x_traint[:training_size],y_train[:training_size],batch_size,steps_per_epoch)\n",
        "    x_test=batch_generator(x_traint[training_size:],\n",
        "                           y_train[training_size:],\n",
        "                           batch_size,\n",
        "                           test_steps_per_epoch)\n",
        "    x_val=batch_generator(x_traint[training_size:training_size+val_size],\n",
        "                          y_train[training_size:training_size+val_size],\n",
        "                          batch_size,\n",
        "                          800)\n",
        "    preds = model.predict_generator(x_test)\n",
        "    print(len(preds),end='')\n",
        "    global test_len\n",
        "    global test_len2\n",
        "    test_len=len(preds)\n",
        "    test_len2=test_size\n",
        "    test_labels=y_train[training_size:len(preds)-test_size]\n",
        "    y_pred=1 * (preds > 0.5)\n",
        "    \"\"\"\n",
        "    print('Accuracy score:{}'.format(accuracy_score(test_labels, y_pred)))\n",
        "    print('F1 score:{}'.format(f1_score(test_labels, y_pred)))\n",
        "    print('ROC AUC score:{}'.format(roc_auc_score(test_labels, y_pred)))\"\"\"\n",
        "    print(classification_report(test_labels, y_pred))\n",
        "    y_proba=[]\n",
        "    for i in preds:\n",
        "        y_proba.append([1-i[0],i[0]])\n",
        "    return test_labels,y_pred,y_proba\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USxIecsq-V4S"
      },
      "source": [
        "def test_result_glove(model):\n",
        "    global y_train\n",
        "    global x_traint\n",
        "    batch_size=70\n",
        "    training_size=int((len(y_train)*70)/100)\n",
        "    val_size=int((training_size*20)/100)\n",
        "    test_size=int((len(y_train)*30)/100)\n",
        "    steps_per_epoch=int(training_size/batch_size)\n",
        "    val_steps_per_epoch=int(val_size/batch_size)\n",
        "    test_steps_per_epoch=int(test_size/batch_size)\n",
        "    x_train=batch_generator_glove(x_traint[:training_size],y_train[:training_size],batch_size,steps_per_epoch)\n",
        "    x_test=batch_generator_glove(x_traint[training_size:],\n",
        "                           y_train[training_size:],\n",
        "                           batch_size,\n",
        "                           test_steps_per_epoch)\n",
        "    x_val=batch_generator_glove(x_traint[training_size:training_size+val_size],\n",
        "                          y_train[training_size:training_size+val_size],\n",
        "                          batch_size,\n",
        "                          800)\n",
        "    preds = model.predict_generator(x_test)\n",
        "    test_labels=y_train[training_size:len(preds)-test_size]    \n",
        "    y_pred=1 * (preds > 0.5)\n",
        "    \"\"\"\n",
        "    print('Accuracy score:{}'.format(accuracy_score(test_labels, y_pred)))\n",
        "    print('F1 score:{}'.format(f1_score(test_labels, y_pred)))\n",
        "    print('ROC AUC score:{}'.format(roc_auc_score(test_labels, y_pred)))\"\"\"\n",
        "    print(classification_report(test_labels, y_pred))\n",
        "    y_proba=[]\n",
        "    for i in preds:\n",
        "        y_proba.append([1-i[0],i[0]])\n",
        "    return test_labels,y_pred,y_proba,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLNVHkw4-QRW"
      },
      "source": [
        "epochs=1\n",
        "model = layers.Input(shape=(100, len(fl)))\n",
        "x = layers.Conv1D(64, 3,activation='relu')(model)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPool1D(3)(x)\n",
        "x = layers.Conv1D(64, 5, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPool1D(5)(x)\n",
        "x = layers.Conv1D(64, 5, activation='relu')(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(100, activation='relu',\n",
        "                  bias_regularizer=regularizers.l1(1e-3), \n",
        "                  activity_regularizer=regularizers.l2(1e-2))(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid',\n",
        "                            bias_regularizer=regularizers.l1(1e-3),\n",
        "                            activity_regularizer=regularizers.l1(1e-3))(x)\n",
        "model1 = models.Model(inputs=model, outputs=predictions)\n",
        "model1.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model1=training_process(model1)\n",
        "  training_acc.extend(model1.history.history['accuracy'])\n",
        "  val_acc.extend(model1.history.history['val_accuracy'])\n",
        "  training_loss.extend(model1.history.history['loss'])\n",
        "  val_loss.extend(model1.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch') \n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "y_test1,y_pred1,y_proba1=test_result(model1)\n",
        "#print(y_test,y_pred,y_proba) \n",
        "ploting(y_pred1,y_test1,y_proba1,'Conv + word2vec Embedding')\n",
        "print('Model1 exicuted')\n",
        "\n",
        "\n",
        "model6 = layers.Input(shape=(100, len(fl)))\n",
        "x = layers.Conv1D(64, 3,activation='relu')(model6)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPool1D(3)(x)\n",
        "x = layers.Conv1D(64, 5, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.MaxPool1D(5)(x)\n",
        "x = layers.Conv1D(64, 5, activation='relu')(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(100, activation='relu',\n",
        "                  bias_regularizer=regularizers.l1(1e-3), \n",
        "                  activity_regularizer=regularizers.l2(1e-2))(x)\n",
        "\n",
        "predictions = layers.Dense(1, activation='sigmoid', \n",
        "\n",
        "                  bias_regularizer=regularizers.l1(1e-3),\n",
        "                  activity_regularizer=regularizers.l1(1e-3))(x)\n",
        "model6 = models.Model(inputs=model6, outputs=predictions)\n",
        "model6.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model6=training_process_glove(model6)\n",
        "  training_acc.extend(model6.history.history['accuracy'])\n",
        "  val_acc.extend(model6.history.history['val_accuracy'])\n",
        "  training_loss.extend(model6.history.history['loss'])\n",
        "  val_loss.extend(model6.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "\n",
        "y_test6,y_pred6,y_proba6=test_result_glove(model6)\n",
        "#print(y_test,y_pred,y_proba)\n",
        "ploting(y_pred6,y_test6,y_proba6,'Conv + Glove Embedding')\n",
        "\"\"\"\n",
        "y_test6,y_pred6,y_proba6=test_result_glove(model6)\n",
        "#print(y_test,y_pred,y_proba) \n",
        "ploting(y_pred6,y_test6,y_proba6,'Conv + Glove Embedding')\"\"\"\n",
        "print('Model 2 (6) exicuted')\n",
        "\n",
        "\n",
        "\n",
        "model2 = layers.Input(shape=(100,len(fl)))\n",
        "x = layers.GRU(128, return_sequences=True)(model2)\n",
        "x = layers.GRU(128)(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "model2 = models.Model(inputs=model2, outputs=predictions)\n",
        "model2.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model2=training_process(model2)\n",
        "  training_acc.extend(model2.history.history['accuracy'])\n",
        "  val_acc.extend(model2.history.history['val_accuracy'])\n",
        "  training_loss.extend(model2.history.history['loss'])\n",
        "  val_loss.extend(model2.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "\n",
        "y_test2,y_pred2,y_proba2=test_result(model2) \n",
        "#print(y_test,y_pred,y_proba) \n",
        "ploting(y_pred2,y_test2,y_proba2,'GRU + word2vec Embedding')\n",
        "print('model 3 (2)exicuted')\n",
        "\n",
        "\n",
        "model3 = layers.Input(shape=(100,len(fl)))\n",
        "x = layers.LSTM(128, return_sequences=True)(model3)\n",
        "x = layers.LSTM(128)(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "model3 = models.Model(inputs=model3, outputs=predictions)\n",
        "model3.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model3=training_process(model3)\n",
        "  training_acc.extend(model3.history.history['accuracy'])\n",
        "  val_acc.extend(model3.history.history['val_accuracy'])\n",
        "  training_loss.extend(model3.history.history['loss'])\n",
        "  val_loss.extend(model3.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "y_test3,y_pred3,y_proba3=test_result(model3)\n",
        "\n",
        "#print(y_test,y_pred,y_proba) \n",
        "ploting(y_pred3,y_test3,y_proba3,'LSTM + word2vec Embedding')\n",
        "print('model 4 (3) exicuted :')\n",
        "\n",
        "model7 = layers.Input(shape=(100,len(fl)))\n",
        "x = layers.GRU(128, return_sequences=True)(model7)\n",
        "x = layers.GRU(128)(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "model7 = models.Model(inputs=model7, outputs=predictions)\n",
        "model7.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model7=training_process_glove(model7)\n",
        "  training_acc.extend(model7.history.history['accuracy'])\n",
        "  val_acc.extend(model7.history.history['val_accuracy'])\n",
        "  training_loss.extend(model7.history.history['loss'])\n",
        "  val_loss.extend(model7.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "y_test7,y_pred7,y_proba7=test_result_glove(model7)\n",
        "#print(y_test,y_pred,y_proba)\n",
        "ploting(y_pred7,y_test7,y_proba7,'GRU + Glove Embedding')\n",
        "print('Model 5(7) exicuted')\n",
        "\n",
        "model8 = layers.Input(shape=(100,len(fl)))\n",
        "x = layers.LSTM(128, return_sequences=True)(model8)\n",
        "x = layers.LSTM(128)(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "model8 = models.Model(inputs=model8, outputs=predictions)\n",
        "model8.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model8=training_process_glove(model8)\n",
        "  training_acc.extend(model8.history.history['accuracy'])\n",
        "  val_acc.extend(model8.history.history['val_accuracy'])\n",
        "  training_loss.extend(model8.history.history['loss'])\n",
        "  val_loss.extend(model8.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "y_test8,y_pred8,y_proba8=test_result_glove(model8)\n",
        "#print(y_test,y_pred,y_proba)\n",
        "ploting(y_pred8,y_test8,y_proba8 ,'LSTM + Glove Embedding')\n",
        "print('model 6 (8) exicuted')\n",
        "\n",
        "model4 = layers.Input(shape=(100,len(fl)))\n",
        "x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(model4)\n",
        "x = layers.Bidirectional(layers.GRU(128))(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x) \n",
        "model4 = models.Model(inputs=model4, outputs=predictions)\n",
        "model4.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model4=training_process(model4)\n",
        "  training_acc.extend(model4.history.history['accuracy'])\n",
        "  val_acc.extend(model4.history.history['val_accuracy'])\n",
        "  training_loss.extend(model4.history.history['loss'])\n",
        "  val_loss.extend(model4.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "y_test4,y_pred4,y_proba4=test_result(model4)\n",
        "#print(y_test,y_pred,y_proba) \n",
        "ploting(y_pred4,y_test4,y_proba4,'Bidirectional GRU + word2vec Embedding')\n",
        "print('model 7 (4) exicuted')\n",
        "\n",
        "model9 = layers.Input(shape=(100,len(fl))) \n",
        "x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(model9)\n",
        "x = layers.Bidirectional(layers.GRU(128))(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "model9 = models.Model(inputs=model9, outputs=predictions)\n",
        "model9.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model9=training_process_glove(model9)\n",
        "  training_acc.extend(model9.history.history['accuracy'])\n",
        "  val_acc.extend(model9.history.history['val_accuracy'])\n",
        "  training_loss.extend(model9.history.history['loss'])\n",
        "  val_loss.extend(model9.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "y_test9,y_pred9,y_proba9=test_result_glove(model9)\n",
        "#print(y_test,y_pred,y_proba)\n",
        "ploting(y_pred9,y_test9,y_proba9,'Bidirectional GRU + Glove Embedding')\n",
        "print('model 8 (9) exicuted')\n",
        "\n",
        "\n",
        "model5 = layers.Input(shape=(100,len(fl)))\n",
        "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(model5)\n",
        "x = layers.Bidirectional(layers.LSTM(128))(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "model5 = models.Model(inputs=model5, outputs=predictions)\n",
        "model5.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model5=training_process(model5)\n",
        "  training_acc.extend(model5.history.history['accuracy'])\n",
        "  val_acc.extend(model5.history.history['val_accuracy'])\n",
        "  training_loss.extend(model5.history.history['loss'])\n",
        "  val_loss.extend(model5.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "y_test5,y_pred5,y_proba5=test_result(model5)\n",
        "#print(y_test,y_pred,y_proba)  \n",
        "ploting(y_pred5,y_test5,y_proba5,'Bidirectional LSTM + word2vec Embedding')\n",
        "\n",
        "print('model 9(5) exicuted')\n",
        "\n",
        "\n",
        "model10 = layers.Input(shape=(100,len(fl)))\n",
        "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(model10)\n",
        "x = layers.Bidirectional(layers.LSTM(128))(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(100, activation='relu')(x)\n",
        "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "model10 = models.Model(inputs=model10, outputs=predictions)\n",
        "model10.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "training_acc=[]\n",
        "training_loss=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "for i in range(epochs):\n",
        "  model10=training_process_glove(model10)\n",
        "  training_acc.extend(model10.history.history['accuracy'])\n",
        "  val_acc.extend(model10.history.history['val_accuracy'])\n",
        "  training_loss.extend(model10.history.history['loss'])\n",
        "  val_loss.extend(model10.history.history['val_loss'])\n",
        "# summarize history for accuracy\n",
        "\"\"\"\n",
        "plt.plot(training_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(training_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\"\"\"\n",
        "y_test10,y_pred10,y_proba10=test_result_glove(model10)\n",
        "#print(y_test,y_pred,y_proba)\n",
        "ploting(y_pred10,y_test10,y_proba10,'Bidirectional LSTM + Glove Embedding')\n",
        "print('model 10 (10) exicuted')\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "st=CountVectorizer(stop_words='english',max_features=28000,lowercase=True) \n",
        "st.fit(x_traint) \n",
        "x_train_c= st.fit_transform(x_traint)\n",
        "\n",
        "from sklearn import linear_model\n",
        "estimator = linear_model.SGDClassifier(loss='log')\n",
        "estimator.fit(x_train_c[:70000], y_train[:70000])\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred11 = estimator.predict(x_train_c[70000:])\n",
        "y_proba11=estimator.predict_proba(x_train_c[70000:])\n",
        "from sklearn.metrics import classification_report   #for calculating precision recall f1-score abd support\n",
        "print(classification_report(y_train[70000:test_len-test_len2],y_pred11[:test_len-test_len2]))\n",
        "ax = plt.axes()\n",
        "\n",
        "y_test11=y_train[70000:test_len-test_len2] \n",
        "ploting(y_pred11[:test_len-test_len2],y_test11,y_proba11[:test_len-test_len2],'SGD + Count Vectorizer')\n",
        "final_y_pred=[]\n",
        "final_y_proba=[]\n",
        "\n",
        "for i in range(len(y_pred2)):\n",
        "    final_y_proba.append([(y_proba1[i][0]+y_proba2[i][0]+y_proba3[i][0]+y_proba4[i][0]+y_proba5[i][0]+y_proba6[i][0]+y_proba7[i][0]+y_proba8[i][0]+y_proba9[i][0]+y_proba10[i][0]+y_proba11[i][0])/11\n",
        "                          ,(y_proba1[i][1]+y_proba2[i][1]+y_proba3[i][1]+y_proba4[i][1]+y_proba5[i][1]+y_proba6[i][1]+y_proba7[i][1]+y_proba8[i][1]+y_proba9[i][1]+y_proba10[i][1]+y_proba11[i][1])/11])\n",
        "    x_n=sum(y_pred1[i]+y_pred2[i]+y_pred3[i]+y_pred4[i]+y_pred5[i]+y_pred6[i]+y_pred7[i]+y_pred8[i]+y_pred9[i]+y_pred10[i]+y_pred11[i])\n",
        "\n",
        "    if (x_n>=6):\n",
        "        final_y_pred.append(1)\n",
        "    else:\n",
        "        final_y_pred.append(0)\n",
        "ploting(final_y_pred,y_train[70000:test_len-test_len2],final_y_proba,'Proposed Ensemble Classifier')\n",
        "probas_list=[y_proba1,y_proba2,y_proba3,y_proba4,y_proba5 ,y_proba6,y_proba7,y_proba8,y_proba9,y_proba10,y_proba11,final_y_proba]\n",
        "clf_names=['Conv+Word2vec+polarity','Conv+Glove+polarity','GRU+Word2vec+polarity','GRU+Glove+polarity',\n",
        "           'LSTM+Word2vec+polarity','LSTM+Glove+polarity','Bidirectional GRU+Word2vec+polarity','Bidirectional GRU+Glove+polarity',\n",
        "           'Bidirectional LSTM+Word2vec+polarity','Bidirectional LSTM+Glove+polarity','SGD+Count Vectorizer','Final ensemble Model']\n",
        "\n",
        "skplt.metrics.plot_calibration_curve(y_train[70000:test_len-test_len2],probas_list,clf_names,figsize=(10,10))\n",
        "j=0\n",
        "for i in probas_list:\n",
        "  skplt.metrics.plot_cumulative_gain(y_train[70000:test_len-test_len2], i,title='Cumulative Gain Curve For : ' +clf_names[j])\n",
        "  j+=1\n",
        "plt.show() \n",
        "\n",
        "j=0\n",
        "for i in probas_list:\n",
        "  skplt.metrics.plot_lift_curve(y_train[70000:test_len-test_len2], i,title='Lift Curve for : '+clf_names[j])\n",
        "  j+=1\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}